{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72166281-7ad2-4e1f-9ca8-6d5f56b019bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from model import Model_args, GPT\n",
    "import time\n",
    "import lzma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40e37684-87d1-4277-9161-a1553a46d400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model parameters\n",
    "block_size = 128\n",
    "batch_size = 32\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embed = 768\n",
    "bias = False\n",
    "dropout = 0.1\n",
    "dataset_path = '/path/to/your/data'\n",
    "init_from = 'scratch'\n",
    "checkpoint_save_dir = '/path/to/checkpoints'\n",
    "eval_iters = 200\n",
    "eval_interval = 10000\n",
    "\n",
    "#Learning rate decay\n",
    "learning_rate = 2e-4\n",
    "warmup_iters = 2000\n",
    "lr_decay_iters = 100000  #num of iterations to decay the learning rate\n",
    "min_lr = 6e-5\n",
    "\n",
    "#Optimizer parameters\n",
    "max_iters = 300000\n",
    "weight_decay = 1e-1\n",
    "betas = (0.9, 0.95)\n",
    "grad_clip = 1.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b91d5a-c8c3-4510-9f8a-b65eb21c1836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#System settings\n",
    "device = 'cuda'\n",
    "device_type = 'cuda'\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52529a8c-3766-4575-83dd-ac272aaf33ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate data from all subfolders\n",
    "'''\n",
    "This block is used when your data is in different folders.\n",
    "I use this because when I tried to use openwebtext data, \n",
    "putting all the data in one folder caused kernel to shut down unexpectedly.\n",
    "'''\n",
    "def aggregate_data(split):\n",
    "    data = []\n",
    "    file_count = 0\n",
    "    for root, dirs, files in os.walk(dataset_path):\n",
    "        for file in files:\n",
    "            if file == f'{split}.bin':\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_data = np.memmap(file_path, dtype=np.uint16, mode='r')\n",
    "                data.append(file_data)\n",
    "                file_count += 1\n",
    "    concatenated_data = np.concatenate(data)\n",
    "    print(f\"Aggregated {file_count} files for {split} with total size: {concatenated_data.shape[0]}\")\n",
    "    return concatenated_data\n",
    "\n",
    "train_data = aggregate_data('train')\n",
    "val_data = aggregate_data('val')\n",
    "\n",
    "def get_batch(split):\n",
    "    if split == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = val_data\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "            #Randomly select start indices for each sequence\n",
    "    x = torch.stack([torch.from_numpy(data[i:i + block_size].astype(np.int64)) for i in ix])\n",
    "            #Create input sequences\n",
    "    y = torch.stack([torch.from_numpy(data[i + 1:i + 1 + block_size].astype(np.int64)) for i in ix])\n",
    "            #Create target sequences\n",
    "    x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "            #Pass to GPU\n",
    "    return x, y\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embed=n_embed, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout)\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "assert init_from == 'scratch' or init_from == 'resume'    #two diffrent task, train from scratch or finetune\n",
    "if init_from == 'scratch':\n",
    "    print(\"Training model from scratch\")\n",
    "    model_args['vocab_size'] = 50304\n",
    "    gpt_args = Model_args(**model_args)\n",
    "    model = GPT(gpt_args)\n",
    "elif init_from == 'resume':\n",
    "    print(\"Resuming training\")\n",
    "    ckpt_path = os.path.join(checkpoint_save_dir, 'checkpoint.pt')\n",
    "    checkpoint = torch.load(ckpt_path, map_location=device)\n",
    "    checkpoint_model_args = checkpoint['model_args']\n",
    "    for k in ['n_layer', 'n_head', 'n_embed', 'block_size', 'bias', 'vocab_size']:\n",
    "        model_args[k] = checkpoint_model_args[k]\n",
    "    gpt_args = Model_args(**model_args)\n",
    "    model = GPT(gpt_args)\n",
    "    state_dict = checkpoint['model']\n",
    "    model.load_state_dict(state_dict)\n",
    "    iter_num = checkpoint['iter_num']\n",
    "    best_val_loss = checkpoint['best_val_loss']\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "\n",
    "model.to(device)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, betas, device_type)\n",
    "if init_from == 'resume':\n",
    "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "checkpoint = None\n",
    "\n",
    "def estimate_loss():\n",
    "    model.eval()\n",
    "    out = {}\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(now_iter):\n",
    "    if now_iter < warmup_iters:\n",
    "        return learning_rate * now_iter / warmup_iters\n",
    "    elif now_iter > lr_decay_iters:\n",
    "        return min_lr\n",
    "    else:\n",
    "        rate = (now_iter - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "        return min_lr + 0.5 * (1.0 + math.cos(math.pi * rate)) * (learning_rate - min_lr)\n",
    "\n",
    "X, Y = get_batch('train')\n",
    "t_before = time.time()\n",
    "\n",
    "while True:\n",
    "    lr = get_lr(iter_num)     #get current learning rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num > 0 and iter_num % eval_interval == 0:\n",
    "        loss_dict = estimate_loss()\n",
    "        print(f\"Iteration {iter_num}, train loss: {loss_dict['train']}, val loss: {loss_dict['val']}\")\n",
    "        best_val_loss = min(loss_dict['val'], best_val_loss)\n",
    "        checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'model_args': model_args,\n",
    "            'iter_num': iter_num,\n",
    "            'best_val_loss': best_val_loss\n",
    "        }\n",
    "        torch.save(checkpoint, os.path.join(checkpoint_save_dir, 'checkpoint.pt'))   #save checkpoint\n",
    "        print(f\"Checkpoint saved at {checkpoint_save_dir}/checkpoint.pt\")\n",
    "    \n",
    "    with ctx:\n",
    "        logits, loss = model(X, Y)\n",
    "        #print(f\"Iteration {iter_num}, loss: {loss.item()}\")\n",
    "            #to notify and stop training when occurs vanishing/exploding gradients.\n",
    "        if torch.isnan(loss) or torch.isinf(loss):       \n",
    "            print(\"Loss is NaN or Inf. Stopping training.\")\n",
    "            break\n",
    "        if iter_num % 1000 == 0:\n",
    "            print(f\"Iteration {iter_num}, loss: {loss.item()}\")\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if grad_clip > 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "    \n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t_after = time.time()\n",
    "    dt = t_after - t_before\n",
    "    t_before = t_after\n",
    "\n",
    "    iter_num += 1\n",
    "    if iter_num > max_iters:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7500c485-4458-4aef-8394-21792e4576fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
