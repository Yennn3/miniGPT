{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e1e01-8c91-42d3-a17d-ed260cf87561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "import inspect\n",
    "\n",
    "from dataclasses import dataclass\n",
    "@dataclass\n",
    "class Model_args:\n",
    "    block_size: int = 1024    #the num of max input\n",
    "    vocab_size: int = 50304\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embed: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True        #whether to use bias in Linear and LayerNorm layers\n",
    "                             #True:bias in Linears and LayerNorms,like GPT-2. False:faster\n",
    "\n",
    "class flash_att(nn.Module):  #same as NanoGPT\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "            #combined q,k,v into one linear\n",
    "        self.qkv_atten = nn.Linear(args.n_embed, 3*args.n_embed, bias=args.bias)\n",
    "            \n",
    "        self.n_head = args.n_head\n",
    "        self.n_embed = args.n_embed\n",
    "        assert args.n_embed % args.n_head == 0       #embedding size is divisible by the number of heads\n",
    "        self.head_size = args.n_embed // args.n_head\n",
    "        self.dropout = args.dropout\n",
    "        self.att_dropout = nn.Dropout(self.dropout)\n",
    "        self.c_proj = nn.Linear(self.n_embed, self.n_embed, bias=args.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape               #batch size (B), sequence length (T), and embedding size (C)\n",
    "        q, k, v = self.qkv_atten(x).split(self.n_embed, dim=2)    #compute query, key, and value vectors\n",
    "            #Reshape for multi-head attention and transpose dimensions for matrix multiplication\n",
    "        q = q.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        k = k.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, self.head_size).transpose(1, 2)\n",
    "        \n",
    "            #use flash attention of pytorch\n",
    "        y = nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None,\n",
    "                                                       dropout_p=self.dropout if self.training else 0,\n",
    "                                                       is_causal=True)\n",
    "            #transpose back and reshape to original dimensions    \n",
    "        y = y.transpose(1, 2)\n",
    "        y = y.contiguous().view(B, T, C)\n",
    "        \n",
    "        return self.att_dropout(self.c_proj(y))\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(args.dropout)\n",
    "        self.up_proj = nn.Linear(args.n_embed, 4*args.n_embed, bias=args.bias)\n",
    "        self.down_c_proj = nn.Linear(4*args.n_embed, args.n_embed, bias=args.bias)\n",
    "        self.act_func = nn.GELU()    #Gelu may be better\n",
    "        self.gate = nn.Linear(args.n_embed, 4*args.n_embed, bias=args.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_proj = self.gate(x)\n",
    "        \n",
    "        x = self.up_proj(x)\n",
    "        x = self.act_func(gate_proj) * x\n",
    "        x = self.down_c_proj(x)\n",
    "        return self.dropout(x)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(args.n_embed, eps=1e-5)\n",
    "        self.attn = flash_att(args)        #Flash attention layer\n",
    "        self.mlp = MLP(args)               #MLP layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm(x))\n",
    "        return x + self.mlp(self.norm(x))\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.args = args\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte=nn.Embedding(args.vocab_size, args.n_embed),  #Token embedding layer\n",
    "            wpe=nn.Embedding(args.block_size, args.n_embed),  #Position embedding layer\n",
    "            drop=nn.Dropout(args.dropout),\n",
    "            h=nn.ModuleList([Block(args) for _ in range(args.n_layer)]),  #Transformer blocks\n",
    "            norm=nn.LayerNorm(args.n_embed, eps=1e-5)\n",
    "        ))\n",
    "\n",
    "        self.lm_head = nn.Linear(args.n_embed, args.vocab_size, bias=False)   #Output layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "            #Calculate total number of parameters\n",
    "        n_sum = 0     \n",
    "        for pname, p in self.named_parameters():\n",
    "            n_sum = n_sum + p.numel()\n",
    "            if pname.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2*args.n_layer))\n",
    "\n",
    "        print(f\"parameters of allï¼š{n_sum}\")\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        B, T = idx.size()          #batch size and sequence length\n",
    "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
    "\n",
    "        token_embed = self.transformer.wte(idx)\n",
    "        pos_embed = self.transformer.wpe(pos)\n",
    "        \n",
    "        x = self.transformer.drop(token_embed + pos_embed)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.norm(x)\n",
    "            #calculate loss\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}  #get parameters that require gradients\n",
    "        decay_params = [p for pn, p in param_dict.items() if p.dim() >= 2]   #parameters for weight decay\n",
    "        nodecay_params = [p for pn, p in param_dict.items() if p.dim() < 2]  #parameters without weight decay\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"parameters using weight decay:{num_decay},parameters without using weight decay:{num_nodecay}\")\n",
    "\n",
    "        fused_avail = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_avail and device_type == 'cuda'\n",
    "        if use_fused:\n",
    "            print(\"AdamW optimiser use fused!\")\n",
    "        extra_args = {'fused': True} if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        \n",
    "        return optimizer\n",
    "\n",
    "    def generate(self, idx, max_generate_tokens, tempreture=1.0, top_k=None):\n",
    "        for _ in range(max_generate_tokens):\n",
    "            idx = idx if idx.shape[1] <= self.args.block_size else idx[:, -self.args.block_size:]\n",
    "            logits, _ = self(idx)\n",
    "            logits = logits[:, -1, :] / tempreture\n",
    "\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)   #sample next token from probabilities\n",
    "            idx = torch.cat((idx, idx_next), dim=1)              #append next token to the sequence\n",
    "\n",
    "        return idx\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
